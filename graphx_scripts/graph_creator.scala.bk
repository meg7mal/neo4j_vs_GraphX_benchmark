import org.apache.spark._
//import org.apache.spark.SparkConf
//import org.apache.spark.SparkContext
//import org.apache.spark.graphx.GraphLoader
//import scala.util.MurmurHash
//import org.apache.spark.graphx.Graph
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD
//import org.apache.spark.graphx.VertexId

object GraphFromFile {
  def main(args: Array[String]) {

    //create SparkContext
    val sparkConf = new SparkConf().setAppName("GraphFromFile").setMaster("local[*]")
    val sc = new SparkContext(sparkConf)

    // read your file
    /*suppose your data is like 
    v1 v3
    v2 v1
    v3 v4
    v4 v2
    v5 v3
    */
    val file = sc.textFile("/tmp/twitter_combined.csv");

    // create edge RDD of type RDD[(VertexId, VertexId)]
   /* val edgesRDD: RDD[(VertexId, VertexId)] = file.map(line => line.split(" "))
      .map(line =>
        (MurmurHash.stringHash(line(0).toString), MurmurHash.stringHash(line(1).toString)))*/

    // create a graph 
//    val graph = Graph.fromEdgeTuples(edgesRDD, 1)

    // you can see your graph 
  //  graph.triplets.collect.foreach(println)

  }
}
